{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-dtOzHyQbFTR"
   },
   "source": [
    "### Train a model on the balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FBbPnWmK5z5W"
   },
   "outputs": [],
   "source": [
    "# Run once to install\n",
    "!pip install image-classifiers==0.2.2\n",
    "!pip install image-classifiers==1.0.0b1\n",
    "!pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3jL0aXh53ml"
   },
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import os \n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import optimizers\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from keras.utils import np_utils\n",
    "from imgaug import augmenters as iaa    \n",
    "import itertools\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Print version\n",
    "print(\"Keras Version\", keras.__version__)\n",
    "print(\"Tensorflow Version\", tf.__version__)\n",
    "\n",
    "\n",
    "# GPU test\n",
    "from tensorflow.python.client import device_lib\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())\n",
    "\n",
    "# Get compute specs\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n",
    "\n",
    "\n",
    "\n",
    "# Helpers functions\n",
    "\n",
    "def create_directory(directory):\n",
    "    '''\n",
    "    Creates a new folder in the specified directory if the folder doesn't exist.\n",
    "    INPUT\n",
    "        directory: Folder to be created, called as \"folder/\".\n",
    "    OUTPUT\n",
    "        New folder in the current directory.\n",
    "    '''\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def plot_hist(img):\n",
    "    \n",
    "    img_flat = img.flatten()\n",
    "    print(min(img_flat), max(img_flat))\n",
    "    \n",
    "    plt.hist(img_flat, bins=20, color='c')\n",
    "    #plt.title(\"Data distribution\")\n",
    "    plt.xlabel(\"Pixel values\")\n",
    "    plt.grid(True)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Focal loss function\n",
    "##################################################################################\n",
    "# Paper: https://arxiv.org/abs/1708.02002\n",
    "\n",
    "#Focal loss down-weights the well-classified examples. This has\n",
    "#the net effect of putting more training emphasis on that data that is hard to classify. \n",
    "#In a practical setting where we have a data imbalance, our majority class will quickly \n",
    "#become well-classified since we have much more data for it. Thus, in order to insure that we\n",
    "#also achieve high accuracy on our minority class, we can use the focal loss to give those minority\n",
    "#class examples more relative weight during training. \n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "# Define paths\n",
    "base_path = os.path.abspath(\"../\")\n",
    "dataset_path = os.path.join(base_path, \"dataset\", \"isic2016numpy\")\n",
    "model_path = os.path.join(base_path, \"models\")\n",
    "print(os.listdir(dataset_path))\n",
    "\n",
    "\n",
    "# Load data\n",
    "x_train = np.load(\"{}/x_upsampled.npy\".format(dataset_path)) \n",
    "y_train = np.load(\"{}/y_upsampled.npy\".format(dataset_path))\n",
    "x_test = np.load(\"{}/x_test.npy\".format(dataset_path))\n",
    "y_test = np.load(\"{}/y_test.npy\".format(dataset_path))\n",
    "\n",
    "\n",
    "# Shuffle training dataset\n",
    "flag = 1\n",
    "if flag == 1:\n",
    "    # Shuffle data\n",
    "    print(\"Shuffling data\")\n",
    "    s = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    x_train = x_train[s]\n",
    "    y_train = y_train[s]\n",
    "else:\n",
    "    print(\"Not shuffling...\")\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# Show shape\n",
    "print(\"Dataset sample size :\", x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# Sanity check on training data\n",
    "#img = x_train[0]\n",
    "#plot_hist(img)\n",
    "\n",
    "#plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mkxe1Mp26ZQi"
   },
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import keras\n",
    "from classification_models.keras import Classifiers\n",
    "\n",
    "# Define architecture\n",
    "arch, preprocess_input = Classifiers.get('vgg16') \n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "\n",
    "# 1. Use model preprocessing\n",
    "#x_train = preprocess_input(x_train)\n",
    "#x_test = preprocess_input(x_test)\n",
    "\n",
    "\n",
    "# 2. Use standard preprocessing\n",
    "prepro = False # False when using synthetic data\n",
    "\n",
    "if prepro == True:\n",
    "    print(\"Preprocessing training data\")\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_train /= 255\n",
    "else:\n",
    "    print(\"Not preprocessing training data, already preprocessed in MeGAN generator.\")\n",
    "    pass\n",
    "\n",
    "# Standardize test set\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "\n",
    "print(x_train.shape, x_test.shape)\n",
    "\n",
    "# Sanity check on preprocessed data\n",
    "#img = x_test[0]\n",
    "#plot_hist(img)\n",
    "#plt.imshow(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "teeONwaw6ZYa"
   },
   "outputs": [],
   "source": [
    "# Experiment name\n",
    "EXP_NAME = \"results\"\n",
    "\n",
    "# Create folder for the experiment\n",
    "create_directory(\"{}/{}\".format(base_path, EXP_NAME))\n",
    "output_path = os.path.join(base_path, EXP_NAME)\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "weights_path = \"{}/{}.h5\".format(output_path, EXP_NAME)\n",
    "checkpointer = ModelCheckpoint(filepath=weights_path, verbose=1, monitor='val_loss', save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_lr=1e-8, mode='auto') # new_lr = lr * factor\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, verbose=1, patience=8, mode='auto', restore_best_weights=True)\n",
    "csv_logger = CSVLogger('{}/{}_training.csv'.format(output_path, EXP_NAME))\n",
    "\n",
    "\n",
    "# Define class weights for imbalacned data\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(np.argmax(y_train, axis=1)), np.argmax(y_train, axis=1))\n",
    "print(class_weights)\n",
    "\n",
    "\n",
    "def my_awesome_model():\n",
    "  \n",
    "    '''Awesomest model'''\n",
    "\n",
    "    # Get backbone network\n",
    "    base_model = arch(input_shape=(256,256,3), weights='imagenet', include_top=False)\n",
    "\n",
    "    # Add GAP layer\n",
    "    x = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    # Add FC layer\n",
    "    output = keras.layers.Dense(2, activation='softmax', trainable=True)(x) \n",
    "\n",
    "    # Freeze layers\n",
    "    #for layer in base_model.layers[:]:\n",
    "    #layer.trainable=False\n",
    "\n",
    "    # Build model\n",
    "    model = keras.models.Model(inputs=[base_model.input], outputs=[output])\n",
    "\n",
    "    # Optimizers\n",
    "    adadelta = optimizers.Adadelta(lr=0.001) \n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer=adadelta, loss= [focal_loss(alpha=.25, gamma=2)], metrics=['accuracy']) \n",
    "\n",
    "    # Output model configuration\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = None\n",
    "model = my_awesome_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F43HGExG6ZbU"
   },
   "outputs": [],
   "source": [
    "# Train the awesome model\n",
    "\n",
    "# Configuration\n",
    "batch_size = 16\n",
    "epochs = 300 \n",
    "\n",
    "# Calculate the starting time    \n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_test, y_test),\n",
    "            class_weight = class_weights,\n",
    "            callbacks=[csv_logger, early_stopping, reduce_lr, checkpointer], # early_stopping, checkpointer, reduce_lr\n",
    "            shuffle=False)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"--- Time taken to train : %s hours ---\" % ((end_time - start_time)//3600))\n",
    "\n",
    "# Save model\n",
    "# If checkpointer is used, dont use this\n",
    "#model.save(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save accuravy loss graphs together\n",
    "def plot_loss_accu_all(history):\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    epochs = range(len(loss))\n",
    "    \n",
    "    plt.plot(epochs, acc, 'r')\n",
    "    plt.plot(epochs, val_acc, 'b')\n",
    "    plt.plot(epochs, loss, 'g')\n",
    "    plt.plot(epochs, val_loss, 'y')\n",
    "    plt.title('Accuracy/Loss')\n",
    "    \n",
    "    #plt.ylabel('Rate')\n",
    "    #plt.xlabel('Epoch')\n",
    "    \n",
    "    plt.legend(['trainacc', 'valacc', 'trainloss', 'valloss'], loc='lower right', fontsize=10)\n",
    "    plt.grid(True)\n",
    "    plt.savefig('{}/{}_acc_loss_graph.jpg'.format(output_path, EXP_NAME), dpi=100)\n",
    "    plt.show()\n",
    "\n",
    "# Plot and save accuravy loss graphs individually\n",
    "def plot_loss_accu(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(loss))\n",
    "    plt.plot(epochs, loss, 'g')\n",
    "    plt.plot(epochs, val_loss, 'y')\n",
    "    #plt.title('Training and validation loss')\n",
    "    plt.ylabel('Loss %')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.grid(True)\n",
    "    #plt.savefig('{}/{}_loss.jpg'.format(output_path, EXP_NAME), dpi=100)\n",
    "    #plt.savefig('{}/{}_loss.pdf'.format(output_path, EXP_NAME), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    loss = history.history['accuracy']\n",
    "    val_loss = history.history['val_accuracy']\n",
    "    epochs = range(len(loss))\n",
    "    plt.plot(epochs, loss, 'r')\n",
    "    plt.plot(epochs, val_loss, 'b')\n",
    "    #plt.title('Training and validation accuracy')\n",
    "    plt.ylabel('Accuracy %')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'val'], loc='lower right')\n",
    "    plt.grid(True)\n",
    "    #plt.savefig('{}/{}_acc.jpg'.format(output_path, EXP_NAME), dpi=100)\n",
    "    #plt.savefig('{}/{}_acc.pdf'.format(output_path, EXP_NAME), dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_accu(model.history)\n",
    "print(\"Done training and logging!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "csbkPKondraG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "train_ISIC_2016.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
